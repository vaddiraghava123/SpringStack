## Work ai in Local setup

# Download and install in local 
  -  'Ollama'

# Pull and run the model of LLAMA 3.2:1b

## Commands of OLLAMa

- To start the ollama
>  Ollama serve

-  To run the model in local
>  ollama run llama3.2:1b

![image](https://github.com/user-attachments/assets/8d314587-cb1c-4b72-9dc6-97a750fd9c3e)

###  Output

![image](https://github.com/user-attachments/assets/d822296d-9cf9-4797-88ef-ddd515d05dae)

